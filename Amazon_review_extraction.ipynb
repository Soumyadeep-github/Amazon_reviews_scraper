{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_review_extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY8s8esIuFKF"
      },
      "source": [
        "################# Isnstallation instructions (for running in Colab) #####################\n",
        "# !pip install defaultlist http_request_randomizer selenium\n",
        "# !apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeXkX6hquFMZ"
      },
      "source": [
        "################# Import Statements #####################\n",
        "from http_request_randomizer.requests.proxy.requestProxy import RequestProxy\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from defaultlist import defaultlist\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "import time\n",
        "from lxml.html import fromstring\n",
        "from selenium import webdriver\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from random import randint\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfjleaYwLv1o"
      },
      "source": [
        "################# Generate product name URL #####################\n",
        "def create_url(product_term, page=1):\n",
        "    \"\"\"Create a url based on search terms. \"\"\"\n",
        "\n",
        "    base_url = \"https://www.amazon.com/\"\n",
        "\n",
        "    product_term = product_term.replace('dp','product-reviews')\n",
        "    # Inserting query   \n",
        "    search_url = base_url+product_term\n",
        "\n",
        "    # Add provision to travel through different pages\n",
        "    if page > 1:\n",
        "      search_url += '/ref=cm_cr_arp_d_paging_btm_next_{}?_encoding=UTF8&pageNumber={}'.format(page,page)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    return search_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDwl3nc3LsO2"
      },
      "source": [
        "################# Prepare the webdriver #####################\n",
        "def set_driver(PROXY=None):\n",
        "  \"\"\"This function is used to set the ChromeWebDriver with all the necessary options like Proxy.\"\"\"\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  if PROXY==None:\n",
        "    driver = webdriver.Chrome('/usr/bin/chromedriver',options=chrome_options)\n",
        "  else:\n",
        "    chrome_options.add_argument('--proxy-server=%s' % PROXY)\n",
        "    driver = webdriver.Chrome('/usr/bin/chromedriver',options=chrome_options)\n",
        "  return driver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pl6GOVnL95-"
      },
      "source": [
        "################# Scraper function #####################\n",
        "def execute_scraper_amazon(driver, ind, product_term, output_path, start=1, end=10):\n",
        "    \"\"\"Run the scraper and display the values as required from amazon. \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    reviews = defaultlist(lambda: 'empty')\n",
        "    loc_date = defaultlist(lambda: 'empty')\n",
        "    page_number = defaultlist(lambda: 'empty')\n",
        "    book = defaultlist(lambda:'empty')\n",
        "    index = defaultlist(lambda:'empty')\n",
        "\n",
        "    file_name = product_term.split('/')[0].replace('-',' ')\n",
        "\n",
        "    #creating the query term & fetching the requested page\n",
        "    url = create_url(product_term)\n",
        "\n",
        "    driver.get(url)\n",
        "    try:\n",
        "      driver.set_page_load_timeout(5)\n",
        "    except:\n",
        "      driver.set_page_load_timeout(10)\n",
        "      driver.get(url)\n",
        "\n",
        "    page_source = driver.page_source\n",
        "    soup = bs(page_source,'html.parser')\n",
        "\n",
        "    time.sleep(randint(20,30))\n",
        "\n",
        "    for i in range(start,end):\n",
        "\n",
        "        if i == 0:\n",
        "          index.append(ind)\n",
        "          pass\n",
        "        else:\n",
        "          index.append(ind+i)\n",
        "          url = create_url(product_term,i)\n",
        "\n",
        "        time.sleep(randint(30,40))\n",
        "\n",
        "        try:\n",
        "          driver.get(url)\n",
        "          driver.set_page_load_timeout(10)\n",
        "        except:\n",
        "          driver.get(url)\n",
        "          driver.set_page_load_timeout(20)   \n",
        "        finally:\n",
        "          driver.get(url)\n",
        "\n",
        "        page_source = driver.page_source\n",
        "        soup = bs(page_source,'html.parser')\n",
        "\n",
        "        if soup.find(\"span\",{\"data-hook\":\"review-body\"}) == None or '':\n",
        "            print('Stopped working')\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print('Working')\n",
        "            time.sleep(randint(30,50))\n",
        "            for j,k in zip(soup.find_all(\"span\",{\"data-hook\":\"review-body\"}),soup.find_all(\"span\",{\"data-hook\":\"review-date\"})):\n",
        "\n",
        "                reviews.append(j.text.strip()) \n",
        "\n",
        "                loc_date.append(k.text.strip())\n",
        "\n",
        "                page_number.append(i)\n",
        "                book.append(file_name)\n",
        "\n",
        "\n",
        "\n",
        "    if len(reviews) > 1 and len(loc_date) > 1:\n",
        "      data['Reviews'] = reviews\n",
        "      data['Location and date'] = loc_date\n",
        "      data['Page'] = page_number\n",
        "      data['Book Name'] = book\n",
        "      data['Index'] = index\n",
        "\n",
        "      df=pd.DataFrame.from_dict(data,orient='index').transpose()\n",
        "      df.to_csv(output_path + file_name + f' {start} to {end}'+'.csv', index=False)\n",
        "\n",
        "      print(\"Data saved\")\n",
        "    else:\n",
        "      print(\"No data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBfzKDaot_Fa"
      },
      "source": [
        "################# Extract data as required #####################\n",
        "def get_data(product_term,ind,start=1,end=20):\n",
        "    \"\"\"This function is the one that performs the extraction. \n",
        "    Since scraping websites like Amazon is not easy it is advisable to \n",
        "    take breaks betweens extractions. That is why 'start' and 'end' variables\n",
        "    are responsible for defining the starting and ending review page numbers.\n",
        "    Post extraction each file created will have the starting and ending pages and there\n",
        "    is another function which can automatically merge all the extracted csvs into 1 file and define \n",
        "    the starting and ending extracting of the whole batch.\"\"\"\n",
        "    proxy = []\n",
        "    print(start)\n",
        "    url = create_url(product_term)\n",
        "    req_proxy = RequestProxy() #you may get different number of proxy when  you run this at each time\n",
        "    proxies = req_proxy.get_proxy_list()\n",
        "    # driver = set_driver(Options())\n",
        "    for i in range(randint(1,5)):\n",
        "      try:\n",
        "        proxy_add = proxies[i].get_address()\n",
        "        driver = set_driver(proxy_add)\n",
        "        driver.get(url);\n",
        "        driver.set_page_load_timeout(10);\n",
        "        proxy.append(proxy_add)\n",
        "      except:\n",
        "        pass\n",
        "    # print(proxy)\n",
        "    # print(proxies)\n",
        "    try:\n",
        "      driver = set_driver(PROXY=proxy[-1])\n",
        "      execute_scraper_amazon(driver,ind,product_term,start=start,end=end)\n",
        "    except:\n",
        "      driver = set_driver()\n",
        "      execute_scraper_amazon(driver,ind,product_term,start=start,end=end)\n",
        "    print(f\"Done for {product_term} from {start} to {end}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imNaDD8D4enE"
      },
      "source": [
        "################# Append CSVs to 1 file #####################\n",
        "def stitch_csvs_together(product_term,file_name_array):\n",
        "  \"\"\"This funciton iteratively stitches the CSVs generated into one file.\"\"\"\n",
        "  merged_csvs = []\n",
        "  # starting_page and ending_page variables hold the values of all the starting and ending pages in all of these files.\n",
        "  starting_page = []\n",
        "  ending_page = []\n",
        "  file_name = product_term.split('/')[0].replace('-',' ')\n",
        "  for i in file_name_array:\n",
        "      extracted = re.search('[\\d]{1,} to [\\d]{1,}',i).group(0)\n",
        "      extracted_list = extracted.split(' ')\n",
        "      starting_page.append(int(extracted_list[0]))\n",
        "      ending_page.append(int(extracted_list[-1]))\n",
        "      csv_to_df = pd.read_csv(f\"/content/{i}\")\n",
        "      merged_csvs.append(csv_to_df)\n",
        "  output_start = min(starting_page)\n",
        "  output_end = max(ending_page)\n",
        "  output_file_name = file_name+' '+str(output_start)+' to '+str(output_end)\n",
        "  print(output_file_name)\n",
        "  merged = pd.concat(merged_csvs)\n",
        "  merged.to_csv(f\"{output_file_name}.csv\", index=False)\n",
        "  print('Files have been merged.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A41DLKLbMUu"
      },
      "source": [
        "################# Calling all the functions one by one. #####################\n",
        "\n",
        "#Some test URLs.\n",
        "test_urls = [\n",
        "\"https://www.amazon.com/Hunger-Games-Book/dp/0439023483\",\n",
        "\"https://www.amazon.com/Last-Wish-Andrzej-Sapkowski/dp/0316029181\",\n",
        "\"https://www.amazon.com/Heard-You-Paint-Houses-Teamsters/dp/1586420771\"]\n",
        "\n",
        "test_product_terms = [i.replace(\"https://www.amazon.com/\",\"\") for i in test_urls]\n",
        "\n",
        "product_term =  test_product_terms[0]\n",
        "#'Hunger-Games-Book/dp/0439023483'\n",
        "\n",
        "#Calls the get_data() function to both scrape data and compile the scraped data into a CSV file.\n",
        "get_data(product_term, ind=301, start=31, end=61)\n",
        "\n",
        "try:\n",
        "  output_path = '/usr/Desktop/' + re.sub(\"\\/dp\\/[\\d]{1,}\", \"\", test_product_terms[0])\n",
        "except:\n",
        "  output_path = input(\"Please enter the complete path where files have been extracted (for each product). \\\n",
        "                      Suppose you have a directory in Desktop with the file name Hunger-Games-Book where \\\n",
        "                      reviews only for that book is being stored then you should enter - /user/Desktop/Hunger-Games-Book (where user is your user name) : \")\n",
        "\n",
        "#The file_name_array list consists of the file names that have been generated.\n",
        "# file_name_array = ['Hunger Games Book 1 to 0.csv', 'Hunger Games Book 2 to 10.csv', 'Hunger Games Book 10 to 31.csv', 'Hunger Games Book 31 to 61.csv']\n",
        "file_name_array = [f for f in listdir(output_path) if isfile(join(output_path, f))]\n",
        "\n",
        "#Calling the stitch_csvs_together() to stitch all files in file_name_array.\n",
        "stitch_csvs_together(product_term,file_name_array)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}